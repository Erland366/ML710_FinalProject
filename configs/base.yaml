train_config:
  lr: 1e-3
  seed: 3407
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 1
  max_seq_length: 512

data_config:
  path: "Erland/fineweb-edu-cleaned-simplified-subset"

parallel_config:
  tp_size: 2
  pp_size: 1
  dp_size: 1

model_config:
  name: "keeeeenw/MicroLlama"