# Plan
- Implement [MicroLLama](https://github.com/keeeeenw/MicroLlama)
- Implement [Picotron](https://github.com/huggingface/picotron) on top of MicroLlama
- Decide on the dataset for training


# Parallel Strategies
- [ ] Data Parallelism
- [ ] Model Parallelism
- [ ] Pipeline Parallelism
- [ ] Data + Model Parallelism (Need at least 4 GPUs)